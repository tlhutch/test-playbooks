import json
import os
import yaml
import time
from collections import namedtuple
from copy import deepcopy

import pytest
from awxkit.exceptions import BadRequest, NoContent

from tests.api import APITest
from tests.api.workflows.utils import (
    get_job_node,
    get_job_status,
)
from tests.lib.helpers.workflow_utils import (WorkflowTree, WorkflowTreeMapper)


def run_wfj_and_assert_completed(
        wfjt,
        test_case_name='',
        wfj=None,
        relaunch=False):
    """Given a workflow job template or a workflow job obejct, launch or relauch and assert all jobs succeed.

    ONLY appropriate for use in workflow job templates where all nodes SHOULD succeed.

    :returns: A worfklow job object.
    """
    if wfj and relaunch:
        wfj = wfj.relaunch().wait_until_completed()
    else:
        wfj = wfjt.launch().wait_until_completed()
    wfj.assert_successful(msg=test_case_name)
    for node in wfj.related.workflow_nodes.get().results:
        job = node.related.job.get()
        assert job.status == 'successful', 'Job in workflow failed during {}'.format(
            test_case_name)
    return wfj


@pytest.mark.usefixtures('authtoken')
class Test_Workflow_Convergence(APITest):
    # Begin Convergence Node tests
    ConvergenceNodeTestCase = namedtuple(
        'ConvergenceNodeTestCase',
        'parent_nodes case_name'
    )

    convergence_node_parent_node_test_cases = [
        ConvergenceNodeTestCase(
            ('n2_success',
             'n3_dnr',
             'n4_always',
             'nf_fail'),
            'All-Type-Parents-Always-Relation'),
        ConvergenceNodeTestCase(
            ('n2_success',
             'nf_fail'),
            'Success-And-Failure-Parents-Always-Relation'),
        ConvergenceNodeTestCase(
            ('n3_dnr',
             'nf_fail'),
            'DNR-And-Failure-Parents-Failure-Relation'),
    ]
    """
    Graphs repersenting the workflows generated by these test cases can be found in the
    tests/api/workflows/convergence_test_case_graphs directory. They are in a format that can be displayed by
    the 'xdot' utility. This is available on fedora from the 'python-xdot' package from dnf.
    """

    # currently clocks on our openshift nodes can be >= 30 seconds out of sync
    # which makes this test not work at all, because start and finish times of
    # jobs are sourced from system clock on pod which may be on different
    # compute nodes
    # Re-visit when we are no longer running on Atomic Host
    @pytest.mark.parametrize(
        'test_case', convergence_node_parent_node_test_cases, ids=[
            case.case_name for case in convergence_node_parent_node_test_cases])
    def test_convergence_node_runs_after_all_parents_reach_definitive_state(
            self, skip_if_openshift, factories, test_case):
        """Confirm that convergence job does not start running until triggering nodes have reached terminal state.

        All parent nodes should have completed or been marked with new do_not_run field before the convergence node
        runs.

        Workflow:

        n0 (uses jt that fails):
          - (always) nf (uses jt that fails)

        n1: (uses jt that succeeds)
          - (success) n2 (uses jt that succeeds)
          - (failure) n3 (uses jt that succeeds)
          - (always)  n4 (uses jt that succeeds)

        Parameterized values:
            Each test case tests a different combination of "parent" nodes for the convergence node.

        Expect:
         - n0 should run and fail
         - n1 should run and succeed
         - n2 should run and succeed
         - n3 should not run and be marked "do not run"
         - n4 should run and succeed
         - nf should run and fail
         - convergence_node should run and succeed
         - convergence_node should not have started until all parents have finished or they have been marked
           "do not run"
        """
        host = factories.host()
        wfjt = factories.workflow_job_template()
        jt = factories.job_template(
            inventory=host.ds.inventory,
            allow_simultaneous=True)
        jt_failure = factories.job_template(
            inventory=host.ds.inventory,
            allow_simultaneous=True,
            playbook='fail_unless.yml')

        n0 = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt_failure)
        n1 = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt)
        n2 = n1.add_success_node(unified_job_template=jt)
        n3 = n1.add_failure_node(unified_job_template=jt)
        n4 = n1.add_always_node(unified_job_template=jt)
        nf = n0.add_always_node(unified_job_template=jt_failure)
        convergence_node = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt)
        parent_nodes = []
        if 'n2_success' in test_case.parent_nodes:
            parent_nodes.append((n2, 'success parent'))
            with pytest.raises(NoContent):
                # this returns a 204, which raises an exception in awxkit
                # because it is not 200 but we are OK with it in this context
                n2.related.success_nodes.post(dict(id=convergence_node.id))
        if 'n3_dnr' in test_case.parent_nodes:
            parent_nodes.append((n3, 'do not run parent'))
            with pytest.raises(NoContent):
                n3.related.always_nodes.post(dict(id=convergence_node.id))
        if 'n4_always' in test_case.parent_nodes:
            parent_nodes.append(
                (n4, 'node that always runs (and succeeds) parent'))
            with pytest.raises(NoContent):
                n4.related.always_nodes.post(dict(id=convergence_node.id))
        if 'nf_fail' in test_case.parent_nodes:
            parent_nodes.append((nf, 'failure node parent'))
            with pytest.raises(NoContent):
                nf.related.failure_nodes.post(dict(id=convergence_node.id))

        # Run the job
        wfj = wfjt.launch().wait_until_completed()
        tree = WorkflowTree(wfjt)
        job_tree = WorkflowTree(wfj)
        mapping = WorkflowTreeMapper(tree, job_tree).map()

        # Assert each job reached expected states
        assert 'failed' == get_job_status(wfj, n0.id, mapping)
        assert 'failed' == get_job_status(wfj, nf.id, mapping)
        assert 'successful' == get_job_status(wfj, n1.id, mapping)
        assert 'successful' == get_job_status(wfj, n2.id, mapping)
        assert 'successful' == get_job_status(wfj, n4.id, mapping)
        assert get_job_node(wfj, n3.id, mapping).do_not_run is True
        assert 'successful' == get_job_status(
            wfj, convergence_node.id, mapping)

        # Assert that all parent jobs ended before convergence node started
        convergence_job = get_job_node(
            wfj, convergence_node.id, mapping).related['job'].get()
        nodes_still_running_when_convergence_job_started = []
        for parent_node_and_name in parent_nodes:
            parent = parent_node_and_name[0]
            description = parent_node_and_name[1]
            parent_job_node = get_job_node(wfj, parent.id, mapping)
            if not parent_job_node.do_not_run:
                parent_job = parent_job_node.related['job'].get()
                # if convergence job started before its parent finished,
                # that is a problem
                if convergence_job.started < parent_job.finished:
                    nodes_still_running_when_convergence_job_started.append(
                        'convergence_node started at {0} which is before parent_job end time {1}, parent job was {2} '.format(
                            convergence_job.started, parent_job.finished, description))

        if nodes_still_running_when_convergence_job_started:
            raise AssertionError(
                'Convergence node started before parent job finisheds. Errors were:\n{}'.format(
                    '\n'.join(nodes_still_running_when_convergence_job_started)))

    def test_convergence_node_runs_as_failure_node_for_canceled_parents(self, factories):
        """Confirm that convergence runs if it is the failure path for canceled parents.

        Workflow:

        n0: (uses jt that succeeds)
          - (failure) dnr (uses jt that would succeed, but should be marked dnr)
        n1: (uses jt that succeeds)
          - (success) n2 (is canceled)
          - (always)  n3 (is canceled)
        n2:
          - (success) convergence_node (uses jt that succeeds)
        n3:
          - (failure) convergence_node (uses jt that succeeds)

        Expect:
         - n0 should run and suceed
         - dnr should be marked dnr
         - n1 should run and succeed
         - n2 should not start and be canceled
         - n3 should not start and be canceled
         - convergence_node should run and succeed
         - convergence_node should not have started until all parents have been canceled
        """
        host = factories.host()
        wfjt = factories.workflow_job_template()
        jt = factories.job_template(
            inventory=host.ds.inventory,
            allow_simultaneous=True)
        jt_sleep = factories.job_template(inventory=host.ds.inventory, playbook='sleep.yml', extra_vars='{"sleep_interval": 30}',
                                          allow_simultaneous=True)  # Longer-running job

        n0 = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt)
        dnr = n0.add_failure_node(unified_job_template=jt)
        n1 = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt)
        n2 = n1.add_success_node(unified_job_template=jt_sleep)
        n3 = n1.add_always_node(unified_job_template=jt_sleep)
        convergence_node = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt)
        with pytest.raises(NoContent):
            # this returns a 204, which raises an exception in awxkit
            # because it is not 200 but we are OK with it in this context
            n2.related.success_nodes.post(dict(id=convergence_node.id))
        with pytest.raises(NoContent):
            n3.related.failure_nodes.post(dict(id=convergence_node.id))

        # Run the job
        wfj = wfjt.launch()
        tree = WorkflowTree(wfjt)
        job_tree = WorkflowTree(wfj)
        mapping = WorkflowTreeMapper(tree, job_tree).map()

        # Cancel jobs of parents of convergence_node
        n2_job_node = wfj.related.workflow_nodes.get(id=mapping[n2.id]).results.pop()
        n3_job_node = wfj.related.workflow_nodes.get(id=mapping[n3.id]).results.pop()
        n2_job_node.wait_for_job(timeout=60)  # Job does not exist until kicked off by workflow
        n2_job = n2_job_node.related.job.get()
        n2_job.cancel()
        n3_job_node.wait_for_job(timeout=60)  # Job does not exist until kicked off by workflow
        n3_job = n3_job_node.related.job.get()
        n3_job.cancel()
        n2_job.wait_until_status('canceled', since_job_created=False)
        n3_job.wait_until_status('canceled', since_job_created=False)

        # Wait for entire workflow to finish
        wfj.wait_until_completed()

        # Assert each job reached expected states
        assert 'successful' == get_job_status(wfj, n0.id, mapping)
        assert 'successful' == get_job_status(wfj, n1.id, mapping)
        assert 'successful' == get_job_status(wfj, convergence_node.id, mapping)
        assert get_job_node(wfj, dnr.id, mapping).do_not_run is True

    def test_dnr_is_propagated(self, factories):
        host = factories.host()
        wfjt = factories.workflow_job_template()
        jt = factories.job_template(
            inventory=host.ds.inventory,
            allow_simultaneous=True)

        root = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt)
        convergence = root.add_success_node(unified_job_template=jt)
        success = root.add_success_node(unified_job_template=jt)
        dnr = success.add_failure_node(unified_job_template=jt)
        with pytest.raises(NoContent):
            dnr.related.failure_nodes.post(dict(id=convergence.id))
        wfj = wfjt.launch().wait_until_completed()
        tree = WorkflowTree(wfjt)
        job_tree = WorkflowTree(wfj)
        mapping = WorkflowTreeMapper(tree, job_tree).map()
        root_job_node = get_job_node(wfj, root.id, mapping)
        root_job_node = root_job_node.get()
        root_job = root_job_node.related['job'].get()
        assert root_job.status == 'successful', 'Root node failed that should have succeeded, cannot proceed'
        success_job_node = get_job_node(wfj, success.id, mapping)
        success_job_node = success_job_node.get()
        success_job = success_job_node.related['job'].get()
        assert success_job.status == 'successful', 'Intermediate node job failed that should have succeeded, cannot proceed'
        # After the "success" node job has completed, the dnr node should be
        # marked DNR
        dnr_job_node = get_job_node(wfj, dnr.id, mapping)
        assert dnr_job_node.do_not_run, "The Do Not Run signal was not correctly propagated"

    supported_workflow_node_types = [
        'project_update',
        'job_template',
        'inventory_sync']

    @pytest.mark.parametrize('node_type', supported_workflow_node_types,
                             ids=supported_workflow_node_types)
    def test_convergence_nodes_may_be_of_any_valid_type(
            self, factories, node_type):
        host = factories.host()
        if node_type == 'project_update':
            unified_jt = factories.project()
        elif node_type == 'job_template':
            unified_jt = factories.job_template(
                inventory=host.ds.inventory, allow_simultaneous=True)
        elif node_type == 'inventory_sync':
            unified_jt = factories.inventory_source()

        wfjt = factories.workflow_job_template()
        parent_jt = factories.job_template(
            playbook='ping.yml', allow_simultaneous=True)
        parent_nodes = []
        for i in range(3):
            parent_node = factories.workflow_job_template_node(
                workflow_job_template=wfjt, unified_job_template=parent_jt
            )
            parent_nodes.append(parent_node)

        # The first parent node we add the convergence node to will return a
        # reference to the convergence node. The rest give us a 204 NoContent
        # response.
        convergence_node = parent_nodes[0].add_always_node(
            unified_job_template=unified_jt)
        for node in parent_nodes[1:]:
            with pytest.raises(NoContent):
                node.related.always_nodes.post(dict(id=convergence_node.id))

        run_wfj_and_assert_completed(
            wfjt, 'Workflow job with convergence node as a {}'.format(node_type))

    def test_convergence_nodes_do_not_allow_cycles(self, factories):
        def assert_cannot_add_self_as_parent(convergence_node):
            """Assertion to make at several stages in building the workflow."""
            # Try and add the convergence node as a parent to itself
            with pytest.raises(BadRequest):
                convergence_node.related.success_nodes.post(
                    dict(id=convergence_node.id))
            with pytest.raises(BadRequest):
                convergence_node.related.always_nodes.post(
                    dict(id=convergence_node.id))
            with pytest.raises(BadRequest):
                convergence_node.related.failure_nodes.post(
                    dict(id=convergence_node.id))

        def assert_cannot_add_parent_as_child(parent, convergence_node):
            """Assertion to make at several stages in building the workflow."""
            # Try and add the convergence node as a parent to one of its
            # parents
            with pytest.raises(BadRequest):
                try:
                    convergence_node.related.failure_nodes.post(
                        dict(id=parent.id))
                except NoContent:
                    raise AssertionError(
                        'Cycle was created where convergence node was allowed to add parent as failure node child.')
            with pytest.raises(BadRequest):
                try:
                    convergence_node.related.success_nodes.post(
                        dict(id=parent.id))
                except NoContent:
                    raise AssertionError(
                        'Cycle was created where convergence node was allowed to add parent as success node child.')
            with pytest.raises(BadRequest):
                try:
                    convergence_node.related.always_nodes.post(
                        dict(id=parent.id))
                except NoContent:
                    raise AssertionError(
                        'Cycle was created where convergence node was allowed to add parent as always node child.')

        jt = factories.job_template()
        wfjt = factories.workflow_job_template()
        ancestor_1 = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt
        )
        ancestor_2 = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt
        )
        parent_nodes_1 = []
        for i in range(2):
            parent_node = factories.workflow_job_template_node(
                workflow_job_template=wfjt, unified_job_template=jt
            )
            parent_nodes_1.append(parent_node)
            with pytest.raises(NoContent):
                ancestor_1.related.always_nodes.post((dict(id=parent_node.id)))

            # This tests disallowing multiple connections from same parent
            # https://github.com/ansible/tower-qa/issues/2528
            with pytest.raises(BadRequest) as exception_obj:
                ancestor_1.related.success_nodes.post(dict(id=parent_node.id))

            assert "Relationship not allowed." in str(exception_obj.value[1]['Error'])

            with pytest.raises(BadRequest) as exception_obj:
                ancestor_1.related.failure_nodes.post(dict(id=parent_node.id))

            assert "Relationship not allowed." in str(exception_obj.value[1]['Error'])

        parent_nodes_2 = []
        for i in range(2):
            parent_node = factories.workflow_job_template_node(
                workflow_job_template=wfjt, unified_job_template=jt
            )
            parent_nodes_2.append(parent_node)
            with pytest.raises(NoContent):
                ancestor_2.related.always_nodes.post((dict(id=parent_node.id)))

        # The first parent node we add the convergence node to will return a
        # reference to the convergence node. The rest give us a 204 NoContent
        # response.
        convergence_node = parent_nodes_1[0].add_always_node(
            unified_job_template=jt)
        for node in parent_nodes_1[1:]:
            with pytest.raises(NoContent):
                node.related.always_nodes.post(dict(id=convergence_node.id))
        assert_cannot_add_self_as_parent(convergence_node)
        parent = parent_nodes_1[0]
        assert_cannot_add_parent_as_child(parent, convergence_node)

        # Add new parents from different ancestor
        for node in parent_nodes_2:
            with pytest.raises(NoContent):
                node.related.success_nodes.post(dict(id=convergence_node.id))

        assert_cannot_add_self_as_parent(convergence_node)
        parent = parent_nodes_2[0]
        assert_cannot_add_parent_as_child(parent, convergence_node)

        # Assert we can add a "sibling" as a triggering node
        # This does not constitute a cycle because the convergence node should
        # wait for all parents to complete before beginning -- so it is not
        # triggered twice.
        sibling_trigger = parent.add_success_node(unified_job_template=jt)
        with pytest.raises(NoContent):
            sibling_trigger.related.always_nodes.post(
                dict(id=convergence_node.id))

        assert_cannot_add_parent_as_child(sibling_trigger, convergence_node)
        parent = parent_nodes_2[0]
        assert_cannot_add_parent_as_child(parent, convergence_node)
        parent = parent_nodes_1[0]
        assert_cannot_add_parent_as_child(parent, convergence_node)
        assert_cannot_add_parent_as_child(ancestor_1, convergence_node)
        assert_cannot_add_parent_as_child(ancestor_2, convergence_node)

    def test_modification_of_workflows_with_convergence_nodes_does_not_break_convergence_and_relaunch_works(
            self,
            factories):
        jt = factories.job_template()
        wfjt = factories.workflow_job_template()
        parent_nodes = []
        for i in range(3):
            parent_node = factories.workflow_job_template_node(
                workflow_job_template=wfjt, unified_job_template=jt
            )
            parent_nodes.append(parent_node)

        # The first parent node we add the convergence node to will return a
        # reference to the convergence node. The rest give us a 204 NoContent
        # response.
        convergence_node = parent_nodes[0].add_always_node(
            unified_job_template=jt)
        for node in parent_nodes[1:]:
            with pytest.raises(NoContent):
                node.related.always_nodes.post(dict(id=convergence_node.id))

        # Add a grandparent to one of the parents
        grandparent = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt
        )
        node_to_delete = parent_nodes.pop()
        with pytest.raises(NoContent):
            grandparent.related.always_nodes.post(dict(id=node_to_delete.id))

        wfj = run_wfj_and_assert_completed(
            wfjt, 'first run of workflow job template before modification')
        # We want to confirm that "grandparents" of convergence node will not trigger the convergence node
        # if the direct parent is deleted. e.g. if we have A -> B -> C and we
        # delete B, A does not trigger C
        node_to_delete.delete()
        bereft_grandparent = grandparent.get()
        assert bereft_grandparent.always_nodes == []
        assert bereft_grandparent.failure_nodes == []
        assert bereft_grandparent.success_nodes == []
        # Assert we can relaunch a job and have it run successfully
        wfj = run_wfj_and_assert_completed(
            wfjt,
            'relaunch of workflow job template after modification',
            wfj,
            relaunch=True)

    def test_convergence_nodes_inherit_success_and_failure_job_set_stats(
            self, factories):
        """This test focuses on just the fact that success and failure jobs should be able to pass on set_stats.

        Merging of data from multiple parents that have shared keys is covered in a seperate test.
        """
        success_vars = {
            'set_stats_data': {
                'success': 'success',
            }
        }
        failure_vars = {
            'set_stats_data': {
                'failure': 'failure',
            },
            'fail_at_end': True
        }
        # create a jt for each set of vars
        wfjt = factories.workflow_job_template()
        host = factories.host()
        failure_jt = factories.job_template(
            inventory=host.ds.inventory,
            allow_simultaneous=True,
            playbook='test_set_stats.yml',
            extra_vars=yaml.dump(failure_vars),
        )
        success_jt = factories.job_template(
            inventory=host.ds.inventory,
            allow_simultaneous=True,
            playbook='test_set_stats.yml',
            extra_vars=yaml.dump(success_vars),
        )
        convergence_jt = factories.job_template(playbook='ping.yml')

        wfjt = factories.workflow_job_template()
        success_parent_node = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=success_jt
        )
        failure_parent_node = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=failure_jt
        )
        parent_nodes = [success_parent_node, failure_parent_node]

        # The first parent node we add the convergence node to will return a
        # reference to the convergence node. The rest give us a 204 NoContent
        # response.
        convergence_node = parent_nodes[0].add_always_node(
            unified_job_template=convergence_jt)
        for node in parent_nodes[1:]:
            with pytest.raises(NoContent):
                node.related.always_nodes.post(dict(id=convergence_node.id))

        wfj = wfjt.launch().wait_until_completed()
        wfj.assert_successful()
        assert wfj.extra_vars == '{}'

        convergence_job = convergence_jt.related.jobs.get().results.pop()
        # get extra vars passed to convergence job
        convergence_vars = json.loads(convergence_job.extra_vars)

        assert convergence_job.status == 'successful', convergence_job.related.stdout.get()

        # Ensure both success and failure job variables got pass on
        expected_convergence_job_vars = {
            'success': 'success',
            'failure': 'failure'
        }
        for key in expected_convergence_job_vars.keys():
            # get vars in state where we can compare them
            assert key in convergence_vars.keys(
            ), 'Expected key {} missing from convergence job variables'.format(key)
            assert convergence_vars[key] == expected_convergence_job_vars[
                key], 'Correct key was inherited by convergence job but found unexpected value'

    def test_convergence_nodes_merge_set_stats_variables(self, instance_group, factories):  # noqa C901
        """Confirm that set stats are merged correctly.

        Correct merging is defined as:
            - All unique keys arrive in final artifact
            - The node with the largest id has highest precedence when merging
              shared keys.
        """
        shared_vars = {
            'set_stats_data': {
                'string': 'A cow jumped over the moon',
                'dictionary': {
                    'a': 1,
                    'barbecue': 'sauce'
                },
                'number': 1,
                'list': [1, 2, 3, 4]
            }
        }
        all_vars_as_python = []

        # this determines the number of parent nodes
        unique_keys = ['zero', 'one', 'two']
        for i in range(len(unique_keys)):
            # each one will have unique values for the shared keys
            v = deepcopy(shared_vars)
            v['set_stats_data']['string'] = str(i)
            v['set_stats_data']['list'] = ['a'] * i
            v['set_stats_data']['dictionary']['a'] = i
            v['set_stats_data']['number'] = i
            # each one will have a unique key
            v['set_stats_data'][unique_keys[i]] = i
            all_vars_as_python.append(v)

        # create a jt for each set of vars
        set_stat_jts = []
        for extra_vars_as_python in all_vars_as_python:
            jt = factories.job_template(
                playbook='test_set_stats.yml',
                extra_vars=yaml.dump(extra_vars_as_python))
            jt.add_instance_group(instance_group)
            set_stat_jts.append(jt)

        convergence_jt = factories.job_template(playbook='ping.yml')

        wfjt = factories.workflow_job_template()
        parent_nodes = []
        for jt in set_stat_jts:
            parent_node = factories.workflow_job_template_node(
                workflow_job_template=wfjt, unified_job_template=jt
            )
            parent_nodes.append(parent_node)

        # want to test that having "grandparents" on some paths does not effect
        # behavior at all
        grandparent_node = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt
        )
        first_parent = parent_nodes[0]
        with pytest.raises(NoContent):
            grandparent_node.related.always_nodes.post(
                dict(id=first_parent.id))

        # The first parent node we add the convergence node to will return a
        # reference to the convergence node. The rest give us a 204 NoContent
        # response.
        convergence_node = parent_nodes[0].add_always_node(
            unified_job_template=convergence_jt)
        last_jt = factories.job_template(playbook='ping.yml')
        last_jt.add_instance_group(instance_group)
        convergence_node.add_always_node(unified_job_template=last_jt)
        for node in parent_nodes[1:]:
            with pytest.raises(NoContent):
                node.related.always_nodes.post(dict(id=convergence_node.id))

        wfj = wfjt.launch().wait_until_completed(timeout=600)
        wfj.assert_successful()
        assert wfj.extra_vars == '{}'

        convergence_job = convergence_jt.related.jobs.get().results.pop()
        last_job = last_jt.related.jobs.get().results.pop()
        # get extra vars passed to convergence job
        convergence_vars = json.loads(convergence_job.extra_vars)
        last_vars = json.loads(last_job.extra_vars)

        tree = WorkflowTree(wfjt)
        job_tree = WorkflowTree(wfj)
        mapping = WorkflowTreeMapper(tree, job_tree).map()
        parent_jobs = []
        for jt in set_stat_jts:
            job = jt.related.jobs.get().results.pop()
            parent_jobs.append(job)

        all_jobs = [convergence_job]
        all_jobs.extend(parent_jobs)
        for job in all_jobs:
            # if any job failed, the rest of the test does not make sense
            assert job.status == 'successful', job.related.stdout.get()

        for i, job in enumerate(parent_jobs):
            job_vars = json.loads(job.extra_vars)
            for key in job_vars['set_stats_data'].keys():
                assert all_vars_as_python[i]['set_stats_data'][key] == job_vars[
                    'set_stats_data'][key], 'set_stats data did not get set correctly in parent job'

        job_nodes = [get_job_node(wfj, node.id, mapping)
                     for node in parent_nodes]
        sorted_job_nodes = sorted(job_nodes, key=lambda node: node.id)
        # Workflow Job Node with highest id, the last to run, will win on collisions
        # We will have to determine what the value of the shared keys will be after the
        # Jobs run
        expected_convergence_job_vars = {
            'string': None,
            'dictionary': None,
            'number': None,
            'list': None,
            'zero': 0,
            'one': 1,
            'two': 2
        }
        winner = sorted_job_nodes[-1]
        winner_vars = json.loads(winner.related.job.get().extra_vars)[
            'set_stats_data']
        for key in winner_vars.keys():
            expected_convergence_job_vars[key] = winner_vars[key]

        # Ensure data got merged in the way expected it to
        merge_errors = []

        for i, v in enumerate(all_vars_as_python):
            # get vars in state where we can compare them
            v = v['set_stats_data']
            assert v.get(
                unique_keys[i]) == i, 'Something went wrong when creating the set_stats for the upstream job'
            if not convergence_vars.get(unique_keys[i]) == v[unique_keys[i]]:
                merge_errors.append(
                    'Unique keys from node {} are missing from the convergence node job extra_vars'.format(i))
            if not last_vars.get(unique_keys[i]) == v[unique_keys[i]]:
                merge_errors.append(
                    'Unique keys from node {} are missing from the last node job extra_vars (child of convergence node)'.format(i))

        # Note: facts are merged based on workflow job template node id.
        # Since we created the nodes in order, the last node we created
        # should have highest precedence for shared keys.
        for key in expected_convergence_job_vars.keys():
            # get vars in state where we can compare them
            assert key in convergence_vars.keys(), "Missing key {} in convergence vars".format(key)
            if isinstance(convergence_vars[key], dict):
                for subkey in convergence_vars[key].keys():
                    if convergence_vars[key][subkey] != expected_convergence_job_vars[key][subkey]:
                        merge_errors.append(
                            'Nested dictionary not inherited correctly in covergence job. Expected {expected} but found {actual}'.format(
                                expected=expected_convergence_job_vars[key][subkey],
                                actual=convergence_vars[key][subkey]))
                # Assert that the child of the convergence node also got
                # correct vars passed down to it.
                for subkey in last_vars[key].keys():
                    if last_vars[key][subkey] != expected_convergence_job_vars[key][subkey]:
                        merge_errors.append(
                            'Nested dictionary not inherited correctly in last job (child of convergence node). Expected {expected} but found {actual}'.format(
                                expected=expected_convergence_job_vars[key][subkey], actual=last_vars[key][subkey]))
            else:
                if convergence_vars[key] != expected_convergence_job_vars[key]:
                    merge_errors.append(
                        'Correct key was inherited by convergence job but found unexpected value.'
                        ' For key {key}, found value {actual} but expected {expected}'.format(
                            key=key,
                            actual=convergence_vars[key],
                            expected=expected_convergence_job_vars[key]))
                if last_vars[key] != expected_convergence_job_vars[key]:
                    merge_errors.append(
                        'Correct key was inherited by last job (child of convergence node) but found unexpected value.'
                        ' For key {key}, found value {actual} but expected {expected}'.format(
                            key=key, actual=last_vars[key], expected=expected_convergence_job_vars[key]))
        assert len(merge_errors) == 0, '\n'.join(merge_errors)

    def test_complex_convergence(self, factories):
        host = factories.host()
        wfjt = factories.workflow_job_template()
        jt = factories.job_template(
            inventory=host.ds.inventory,
            allow_simultaneous=True)
        jt_failure = factories.job_template(
            inventory=host.ds.inventory,
            allow_simultaneous=True,
            playbook='fail_unless.yml')

        l1n1 = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt)
        l1n2 = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt)

        l2n1 = l1n1.related.success_nodes.post(
            dict(unified_job_template=jt.id))
        l2n2 = l1n1.related.failure_nodes.post(
            dict(unified_job_template=jt.id))
        l2n3 = l1n1.related.failure_nodes.post(
            dict(unified_job_template=jt.id))
        l2n4 = l1n1.related.failure_nodes.post(
            dict(unified_job_template=jt.id))
        l2n5 = l1n1.related.failure_nodes.post(
            dict(unified_job_template=jt.id))
        l2n6 = l1n2.related.success_nodes.post(dict(unified_job_template=jt.id))  # noqa
        l2n7 = l1n2.related.success_nodes.post(dict(unified_job_template=jt.id))  # noqa

        l3n1 = l2n1.related.success_nodes.post(
            dict(unified_job_template=jt.id))
        l3n2 = l2n2.related.success_nodes.post(
            dict(unified_job_template=jt.id))

        with pytest.raises(NoContent):
            # this returns a 204, which raises an exception in awxkit
            # because it is not 200 but we are OK with it in this context
            l2n3.related.always_nodes.post(dict(id=l3n2.id))

        l3n3 = l2n4.related.failure_nodes.post(
            dict(unified_job_template=jt.id))

        with pytest.raises(NoContent):
            l2n4.related.failure_nodes.post(dict(id=l3n3.id))
        with pytest.raises(NoContent):
            l1n1.related.failure_nodes.post(dict(id=l3n2.id))
        with pytest.raises(NoContent):
            l2n3.related.always_nodes.post(dict(id=l3n2.id))

        l4n1 = l3n1.related.success_nodes.post(
            dict(unified_job_template=jt.id))
        l4n2 = l3n2.related.failure_nodes.post(
            dict(unified_job_template=jt_failure.id))
        l4n3 = l3n2.related.failure_nodes.post(
            dict(unified_job_template=jt.id))
        l4n4 = l3n3.related.failure_nodes.post(
            dict(unified_job_template=jt.id))
        l4n5 = l3n3.related.failure_nodes.post(
            dict(unified_job_template=jt.id))

        l5n1 = l4n1.related.always_nodes.post(
            dict(unified_job_template=jt_failure.id))

        with pytest.raises(NoContent):
            l5n1.related.failure_nodes.post(dict(id=l4n2.id))

        l5n2 = l4n2.related.failure_nodes.post(dict(unified_job_template=jt.id))  # noqa
        l5n3 = l4n2.related.failure_nodes.post(dict(unified_job_template=jt.id))  # noqa

        wfj = wfjt.launch().wait_until_completed()

        tree = WorkflowTree(wfjt)
        job_tree = WorkflowTree(wfj)
        mapping = WorkflowTreeMapper(tree, job_tree).map()

        # Assert correct nodes get marked do_not_run
        assert get_job_node(wfj, l2n2.id, mapping).do_not_run is True
        assert get_job_node(wfj, l2n3.id, mapping).do_not_run is True
        assert get_job_node(wfj, l2n4.id, mapping).do_not_run is True
        assert get_job_node(wfj, l2n5.id, mapping).do_not_run is True
        assert get_job_node(wfj, l3n3.id, mapping).do_not_run is True
        assert get_job_node(wfj, l3n2.id, mapping).do_not_run is True
        assert get_job_node(wfj, l4n5.id, mapping).do_not_run is True
        assert get_job_node(wfj, l4n4.id, mapping).do_not_run is True
        assert get_job_node(wfj, l4n3.id, mapping).do_not_run is True

        # Assert nodes that run jobs don't have do_not_run set
        assert get_job_node(wfj, l1n1.id, mapping).do_not_run is False
        assert get_job_node(wfj, l1n2.id, mapping).do_not_run is False
        assert get_job_node(wfj, l2n1.id, mapping).do_not_run is False
        assert get_job_node(wfj, l2n6.id, mapping).do_not_run is False
        assert get_job_node(wfj, l2n7.id, mapping).do_not_run is False
        assert get_job_node(wfj, l3n1.id, mapping).do_not_run is False
        assert get_job_node(wfj, l4n2.id, mapping).do_not_run is False
        assert get_job_node(wfj, l4n1.id, mapping).do_not_run is False
        assert get_job_node(wfj, l5n1.id, mapping).do_not_run is False
        assert get_job_node(wfj, l5n2.id, mapping).do_not_run is False
        assert get_job_node(wfj, l5n3.id, mapping).do_not_run is False

        # Assert the status of expected job runs
        assert 'successful' == get_job_status(wfj, l1n1.id, mapping)
        assert 'successful' == get_job_status(wfj, l1n2.id, mapping)

        assert 'successful' == get_job_status(wfj, l2n1.id, mapping)
        assert 'successful' == get_job_status(wfj, l2n6.id, mapping)
        assert 'successful' == get_job_status(wfj, l2n7.id, mapping)

        assert 'successful' == get_job_status(wfj, l3n1.id, mapping)

        assert 'successful' == get_job_status(wfj, l4n1.id, mapping)
        assert 'failed' == get_job_status(wfj, l4n2.id, mapping)

        assert 'failed' == get_job_status(wfj, l5n1.id, mapping)
        assert 'successful' == get_job_status(wfj, l5n2.id, mapping)
        assert 'successful' == get_job_status(wfj, l5n3.id, mapping)

    @pytest.mark.parametrize('order', ['ascending', 'descending'])
    def test_dense_graph_convergence(self, factories, order):
        """Confirm that covergence nodes are triggered properly in a dense graph.

        A dense DAG is a graph where no more connections can be made without creating a cycle.
        The order of construction of the graph should not effect the success of the WFJ.
        """
        NUM_NODES = int(
            os.environ.get(
                'TOWERQA_NUM_NODES_DENSE_CONVERGENCE',
                0))
        if not NUM_NODES:
            pytest.skip(
                'Set TOWERQA_NUM_NODES_DENSE_CONVERGENCE to a positive integer to run test')
        host = factories.host()
        temp_jt = factories.job_template(
            inventory=host.ds.inventory,
            allow_simultaneous=True)
        wfjt = factories.workflow_job_template()
        wfjt_inner = factories.workflow_job_template()
        nodes = []
        for i in range(NUM_NODES):
            # HACK create node with temporary regular job template so it does not create one. Then replace
            # with workflow job template, because awxkit won't let us pass
            # one directly
            node = factories.workflow_job_template_node(
                workflow_job_template=wfjt, unified_job_template=temp_jt)
            node.unified_job_template = wfjt_inner.id
            nodes.append(node)
        if order == 'descending':
            nodes.reverse()
        start = time.time()
        for i, parent in enumerate(nodes):
            for child in nodes[i + 1:]:
                with pytest.raises(NoContent):
                    if i % 3 == 0:
                        parent.related.always_nodes.post(dict(id=child.id))
                    if i % 3 == 1:
                        parent.related.failure_nodes.post(dict(id=child.id))
                    if i % 3 == 2:
                        parent.related.success_nodes.post(dict(id=child.id))
        elapsed = time.time() - start
        print('*******WF took {} to construct using nodes with ids in {} order*******'.format(elapsed, order))
        wfj = wfjt.launch().wait_until_completed(timeout=NUM_NODES * 60)
        assert 'successful' == wfj.status
        print('*******WF took {} to complete*******'.format(wfj.elapsed))

    convergence_node_all_parents_must_converge_test_cases = [
        ConvergenceNodeTestCase(
            ('n0_success',
             'n1_success'),
            'Success-Parents-Success-Relation'),
        ConvergenceNodeTestCase(
            ('n0_success',
             'n2_fail_on_failure_link'),
            'Success-And-Failure-Parents-Failure-Relation'),
        ConvergenceNodeTestCase(
            ('n1_success',
             'n2_fail_on_success_link'),
            'Success-And-Failure-Parents-Success-Relation'),
        ConvergenceNodeTestCase(
            ('n1_success',
             'n3_dnr'),
            'DNR-And-Success-Parents-Always-Relation'),
    ]

    @pytest.mark.parametrize(
        'test_case', convergence_node_all_parents_must_converge_test_cases,
        ids=[case.case_name for case in convergence_node_all_parents_must_converge_test_cases])
    def test_convergence_node_when_all_parents_must_converge_is_set_to_true(
            self, skip_if_openshift, factories, test_case):
        """Confirm that convergence node job runs/skips depending on the scenario
        when all_parents_must_converge property is set to true

        Workflow:

        n0_success (uses jt that succeeds)
        n1_success: (uses jt that succeeds)
        n2_failure: (uses jt that fails)
        n3_dnr: (this node does not run)

        Parameterized values:
            Each test case tests a different combination of "parent" nodes for the convergence node.

        Expect:
         - convergence_node should run and succeed or not run, depending on the scenario
         - convergence_node should not have started until all parents have finished
        """

        host = factories.host()
        wfjt = factories.workflow_job_template()
        jt = factories.job_template(
            inventory=host.ds.inventory,
            allow_simultaneous=True)
        jt_failure = factories.job_template(
            inventory=host.ds.inventory,
            allow_simultaneous=True,
            playbook='fail_unless.yml')

        # Creation of the workflow nodes
        n0_success = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt)
        n1_success = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt)
        n2_failure = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt_failure)
        n3_dnr = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt)
        convergence_node = factories.workflow_job_template_node(
            workflow_job_template=wfjt, unified_job_template=jt)
        # Assert that convergence is set to "ANY" by default
        assert not convergence_node.all_parents_must_converge
        convergence_node.all_parents_must_converge = True # set it to "ALL"
        parent_nodes = []

        # Associate convergence node with it's parents depending upon the scenario
        if 'n0_success' in test_case.parent_nodes:
            parent_nodes.append((n0_success, 'success parent'))
            with pytest.raises(NoContent):
                n0_success.related.success_nodes.post(dict(id=convergence_node.id))
        if 'n1_success' in test_case.parent_nodes:
            parent_nodes.append((n1_success, 'success parent'))
            with pytest.raises(NoContent):
                n1_success.related.success_nodes.post(dict(id=convergence_node.id))
        if 'n2_fail_on_success_link' in test_case.parent_nodes:
            parent_nodes.append((n2_failure, 'failure node parent'))
            with pytest.raises(NoContent):
                n2_failure.related.success_nodes.post(dict(id=convergence_node.id))
        if 'n2_fail_on_failure_link' in test_case.parent_nodes:
            parent_nodes.append((n2_failure, 'failure node parent'))
            with pytest.raises(NoContent):
                n2_failure.related.failure_nodes.post(dict(id=convergence_node.id))
        if 'n3_dnr' in test_case.parent_nodes:
            parent_nodes.append((n3_dnr, 'failure node parent'))
            with pytest.raises(NoContent):
                n3_dnr.related.always_nodes.post(dict(id=convergence_node.id))
            with pytest.raises(NoContent):
                n2_failure.related.success_nodes.post(dict(id=n3_dnr.id))

        # Run the job
        wfj = wfjt.launch().wait_until_completed()
        tree = WorkflowTree(wfjt)
        job_tree = WorkflowTree(wfj)
        mapping = WorkflowTreeMapper(tree, job_tree).map()

        # Assert each job reached expected states
        assert 'successful' == get_job_status(wfj, n0_success.id, mapping)
        assert 'successful' == get_job_status(wfj, n1_success.id, mapping)
        assert 'failed' == get_job_status(wfj, n2_failure.id, mapping)

        convergence_workflow_job_node = get_job_node(wfj, convergence_node.id, mapping)
        assert convergence_workflow_job_node.all_parents_must_converge

        # Assert that in the case where one of the parents is a Failure node or  DNR, the convergence node did not run
        if test_case.case_name in ['Success-And-Failure-Parents-Success-Relation', 'DNR-And-Success-Parents-Always-Relation']:
            assert convergence_workflow_job_node.do_not_run
        # Assert that in other cases, the convergence node ran successfully
        else:
            assert 'successful' == get_job_status(wfj, convergence_node.id, mapping)
            assert not convergence_workflow_job_node.do_not_run

        # Assert that all parent jobs ended before convergence node started
        nodes_still_running_when_convergence_job_started = []
        for parent_node_and_name in parent_nodes:
            parent = parent_node_and_name[0]
            description = parent_node_and_name[1]
            parent_job_node = get_job_node(wfj, parent.id, mapping)
            if not parent_job_node.do_not_run:
                parent_job = parent_job_node.related['job'].get()
                # if convergence job started before its parent finished,
                # that is a problem
                if not convergence_workflow_job_node.do_not_run:
                    convergence_job = get_job_node(
                        wfj, convergence_node.id, mapping).related['job'].get()
                    convergence_job.assert_successful()
                    if convergence_job.started < parent_job.finished:
                        nodes_still_running_when_convergence_job_started.append(
                            'convergence_node started at {0} which is before parent_job end time {1}, parent job was {2} '.format(
                                convergence_job.started, parent_job.finished, description))

        if nodes_still_running_when_convergence_job_started:
            raise AssertionError(
                'Convergence node started before parent job finisheds. Errors were:\n{}'.format(
                    '\n'.join(nodes_still_running_when_convergence_job_started)))
